{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d599efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze simulation data\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import teams.teams_helpers as teams_helpers\n",
    "import params_team as params\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "import os\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "import textwrap\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "import copy\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "import teams.teams_helpers as team_helpers\n",
    "import params_team as params\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "538933e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sensitivity_analysis(path, files, file_prefix_list, runs_to_exclude_list=[], runs_to_analyze_list = [], vars_filename_prefix = ''):\n",
    "\n",
    "    \n",
    "    def convert_to_dict(sim_vars, parameter_combinations):\n",
    "        demo_strategy = sim_vars['demo_strategy'].iloc[0]\n",
    "        team_composition = sim_vars['team_composition'].iloc[0]\n",
    "\n",
    "        study_id = int(sim_vars['study_id'].iloc[0])\n",
    "        run_id = sim_vars['run_no'].iloc[0] \n",
    "\n",
    "        max_learning_factor = np.round(sim_vars['max_learning_factor'].iloc[0],2)\n",
    "        team_learning_factor = sim_vars['initial_team_learning_factor'].iloc[0]\n",
    "        # team_learning_rate = sim_vars['team_learning_rate'].iloc[0]\n",
    "\n",
    "\n",
    "        learning_factor_high_learner = []\n",
    "        learning_factor_low_learner = []\n",
    "        for i in range(len(team_composition)):\n",
    "            if team_composition[i] == 0:\n",
    "                learning_factor_low_learner = np.round(team_learning_factor[i],2)\n",
    "            elif team_composition[i] == 2:\n",
    "                learning_factor_high_learner = np.round(team_learning_factor[i],2)\n",
    "\n",
    "        # for i in range(len(team_learning_rate)):\n",
    "        #     team_learning_rate[i] = np.round(team_learning_rate[i], 2)\n",
    "        \n",
    "\n",
    "        max_loop_count = sim_vars['loop_count'].iloc[-1]\n",
    "        bec_final = sim_vars['BEC_knowledge_level'][len(sim_vars)-1]\n",
    "\n",
    "\n",
    "        \n",
    "        sensitivity_data_dict = {}\n",
    "\n",
    "        sensitivity_data_dict['demo_strategy'] = demo_strategy\n",
    "        sensitivity_data_dict['team_composition'] = team_composition\n",
    "        sensitivity_data_dict['study_id'] = study_id\n",
    "        sensitivity_data_dict['run_id'] = run_id\n",
    "        \n",
    "        sensitivity_data_dict['max_learning_factor'] = max_learning_factor\n",
    "        sensitivity_data_dict['learning_factor_high_learner'] = learning_factor_high_learner\n",
    "        sensitivity_data_dict['learning_factor_low_learner'] = learning_factor_low_learner\n",
    "\n",
    "        sensitivity_data_dict['teacher_learning_factor'] = np.round(parameter_combinations[study_id-1][4],2)\n",
    "        sensitivity_data_dict['learning_rate'] = np.round(parameter_combinations[study_id-1][2],2)\n",
    "        \n",
    "        sensitivity_data_dict['parameter_combinations'] = parameter_combinations[study_id-1]\n",
    "        # sensitivity_data_dict['team_learning_rate'] = team_learning_rate\n",
    "\n",
    "        sensitivity_data_dict['max_loop_count'] = int(max_loop_count)\n",
    "        sensitivity_data_dict['bec_final'] = bec_final\n",
    "\n",
    "\n",
    "        return sensitivity_data_dict\n",
    "\n",
    "    #####################\n",
    "    # read parameter combinations\n",
    "    with open('data/simulation/sim_experiments/sensitivity_analysis/w_feedback/param_combinations_forte.pickle', 'rb') as f:\n",
    "            parameter_combinations = pickle.load(f)\n",
    "\n",
    "    \n",
    "    params_pd = pd.DataFrame()\n",
    "    for i in range(len(parameter_combinations)):\n",
    "        params_dict = {'study_id': i+1, 'learning_factor_low': parameter_combinations[i][0], 'learning_factor_high': parameter_combinations[i][1], 'learning_rate': parameter_combinations[i][2], 'max_learning_factor': parameter_combinations[i][3], 'teacher_learning_factor': parameter_combinations[i][4]}\n",
    "        params_pd = params_pd.append(params_dict, ignore_index=True)\n",
    "\n",
    "    params_pd.to_csv('data/simulation/sim_experiments/sensitivity_analysis/w_feedback/param_combinations_forte.csv')\n",
    "\n",
    "#     print('parameter_combinations:', parameter_combinations)\n",
    "\n",
    "\n",
    "\n",
    "#     #############\n",
    "    run_no = 1\n",
    "    sensitivity_data = pd.DataFrame()\n",
    "    for file in files:\n",
    "\n",
    "        # check if file is a valid file\n",
    "        for file_prefix in file_prefix_list:\n",
    "            if file_prefix in file and '.pickle' in file:\n",
    "                run_file_flag = True\n",
    "                break\n",
    "            else:\n",
    "                run_file_flag = False\n",
    "\n",
    "        # check if file needs to be excluded\n",
    "        for runs_to_exclude in runs_to_exclude_list:\n",
    "            if runs_to_exclude in file:\n",
    "                run_file_flag = False\n",
    "                break\n",
    "\n",
    "        \n",
    "        if run_file_flag:\n",
    "            \n",
    "            with open(path + '/' + file, 'rb') as f:\n",
    "                sim_vars = pickle.load(f)\n",
    "            \n",
    "            print('Reading file: ', file)\n",
    "\n",
    "            # check if there are multiple runs in the same file\n",
    "            loop_count_var = sim_vars['loop_count']\n",
    "            run_change_idx = [idx for idx in range(len(loop_count_var)-1) if loop_count_var[idx] > loop_count_var[idx+1]]\n",
    "\n",
    "#             print('run_change_idx: ', run_change_idx)\n",
    "            if len(run_change_idx) > 0:\n",
    "\n",
    "                for run_idx in range(2):\n",
    "                    if run_idx == 0:\n",
    "                        run_sim_vars = sim_vars.iloc[:run_change_idx[0]+1]\n",
    "                    else:\n",
    "                        run_sim_vars = sim_vars.iloc[run_change_idx[0]+2:]\n",
    "                        \n",
    "                    # reset index\n",
    "                    run_sim_vars = run_sim_vars.reset_index(drop=True)\n",
    "\n",
    "\n",
    "                    bec_final = run_sim_vars['BEC_knowledge_level'][len(run_sim_vars)-1]\n",
    "                    # check if learning was completed\n",
    "                    learning_complete = True\n",
    "                    for k_type, k_val in bec_final.items():\n",
    "                        if k_val[0] != 1:\n",
    "                            learning_complete = False\n",
    "                            break\n",
    "                    \n",
    "                    if learning_complete:\n",
    "                        sensitivity_data_dict = convert_to_dict(run_sim_vars, parameter_combinations)\n",
    "                        sensitivity_data = sensitivity_data.append(sensitivity_data_dict, ignore_index=True)\n",
    "            else:\n",
    "\n",
    "                bec_final = sim_vars['BEC_knowledge_level'][len(sim_vars)-1]\n",
    "                # check if learning was completed\n",
    "                learning_complete = True\n",
    "                for k_type, k_val in bec_final.items():\n",
    "                    if k_val[0] != 1:\n",
    "                        learning_complete = False\n",
    "                        break\n",
    "            \n",
    "                if learning_complete:\n",
    "                    sensitivity_data_dict = convert_to_dict(sim_vars, parameter_combinations)\n",
    "                    sensitivity_data = sensitivity_data.append(sensitivity_data_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "    sensitivity_data.to_csv(path + '/sensitivity_data.csv')\n",
    "    with open(path + '/sensitivity_data.pickle', 'wb') as f:\n",
    "        pickle.dump(sensitivity_data, f)\n",
    "\n",
    "    print(sensitivity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f6c9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Sensitivity Analysis\n",
    "path = 'data/simulation/sim_experiments/sensitivity_analysis/w_feedback/augmented_taxi2'\n",
    "files = os.listdir(path)\n",
    "\n",
    "file_prefix_list = ['trials_02_11_sensitivity_w_feedback_tc1_ck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5e493e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_1_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_3_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_3_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_4_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_5_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_5_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_9_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_10_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_10_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_11_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_12_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_13_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_14_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_15_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_17_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_17_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_17_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_18_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_18_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_18_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_19_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_20_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_20_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_20_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_21_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_22_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_23_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_23_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_25_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_27_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_27_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_27_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_28_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_29_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_30_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_1_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_2_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_3_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_3_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_3_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_4_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_6_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_6_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_7_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_7_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_8_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_8_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_9_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_9_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_9_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_11_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_11_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_14_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_15_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_19_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_20_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_20_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_21_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_22_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_22_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_26_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_29_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_30_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_2_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_2_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_2_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_2_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_4_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_6_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_7_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_7_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_8_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_9_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_10_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_10_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_10_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_12_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_12_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_13_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_13_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_15_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_15_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_16_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_21_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_22_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_24_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_24_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_24_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_25_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_26_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_26_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_27_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_28_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_30_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_1_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_4_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_4_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_5_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_6_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_6_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_8_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_11_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_12_run_4.pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_13_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_13_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_14_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_15_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_16_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_17_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_18_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_19_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_19_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_19_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_21_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_22_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_23_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_23_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_23_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_24_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_25_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_26_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_27_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_28_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_28_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_29_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_29_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_29_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_30_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_1_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_1_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_5_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_5_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_7_run_5.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_8_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_11_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_12_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_14_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_14_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_16_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_16_run_3.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_16_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_17_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_18_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_21_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_24_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_25_run_2.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_25_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_26_run_1.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_28_run_4.pickle\n",
      "Reading file:  trials_02_11_sensitivity_w_feedback_tc1_ck_study_30_run_3.pickle\n",
      "                 demo_strategy team_composition  study_id  run_id  \\\n",
      "0             common_knowledge        [0, 0, 0]         1       4   \n",
      "1    individual_knowledge_high        [0, 0, 0]         1       4   \n",
      "2             common_knowledge        [0, 0, 0]         3       1   \n",
      "3    individual_knowledge_high        [0, 0, 0]         3       1   \n",
      "4             common_knowledge        [0, 0, 0]         3       2   \n",
      "..                         ...              ...       ...     ...   \n",
      "287  individual_knowledge_high        [0, 0, 0]        26       1   \n",
      "288           common_knowledge        [0, 0, 0]        28       4   \n",
      "289  individual_knowledge_high        [0, 0, 0]        28       4   \n",
      "290           common_knowledge        [0, 0, 0]        30       3   \n",
      "291  individual_knowledge_high        [0, 0, 0]        30       3   \n",
      "\n",
      "     max_learning_factor  learning_factor_high_learner  \\\n",
      "0                   0.95                          0.70   \n",
      "1                   0.95                          0.70   \n",
      "2                   0.91                          0.62   \n",
      "3                   0.91                          0.62   \n",
      "4                   0.91                          0.62   \n",
      "..                   ...                           ...   \n",
      "287                 0.92                          0.67   \n",
      "288                 0.93                          0.63   \n",
      "289                 0.93                          0.63   \n",
      "290                 0.87                          0.64   \n",
      "291                 0.87                          0.64   \n",
      "\n",
      "    learning_factor_low_learner  teacher_learning_factor  learning_rate  \\\n",
      "0                            []                     0.85           0.02   \n",
      "1                            []                     0.85           0.02   \n",
      "2                            []                     0.70           0.08   \n",
      "3                            []                     0.70           0.08   \n",
      "4                            []                     0.70           0.08   \n",
      "..                          ...                      ...            ...   \n",
      "287                          []                     0.81           0.04   \n",
      "288                          []                     0.61           0.09   \n",
      "289                          []                     0.61           0.09   \n",
      "290                          []                     0.67           0.04   \n",
      "291                          []                     0.67           0.04   \n",
      "\n",
      "                                parameter_combinations  max_loop_count  \\\n",
      "0    [0.6970222568479975, 0.7996973399018957, 0.016...               7   \n",
      "1    [0.6970222568479975, 0.7996973399018957, 0.016...               7   \n",
      "2    [0.6170143606345991, 0.7845280567178851, 0.081...               6   \n",
      "3    [0.6170143606345991, 0.7845280567178851, 0.081...               8   \n",
      "4    [0.6170143606345991, 0.7845280567178851, 0.081...               9   \n",
      "..                                                 ...             ...   \n",
      "287  [0.6733042565383901, 0.8074503557432919, 0.038...               9   \n",
      "288  [0.6349435886249267, 0.7830230916552369, 0.090...               9   \n",
      "289  [0.6349435886249267, 0.7830230916552369, 0.090...               7   \n",
      "290  [0.6426821347430337, 0.8155619107223626, 0.044...               9   \n",
      "291  [0.6426821347430337, 0.8155619107223626, 0.044...               7   \n",
      "\n",
      "                                             bec_final  \n",
      "0    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "1    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "2    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "3    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "4    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "..                                                 ...  \n",
      "287  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "288  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "289  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "290  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "291  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
      "\n",
      "[292 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "run_sensitivity_analysis(path, files, file_prefix_list, runs_to_exclude_list=[], runs_to_analyze_list = [], vars_filename_prefix = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a0b5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (path + '/sensitivity_data.pickle', 'rb') as f:\n",
    "    sensitivity_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb616f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demo_strategy</th>\n",
       "      <th>team_composition</th>\n",
       "      <th>study_id</th>\n",
       "      <th>run_id</th>\n",
       "      <th>max_learning_factor</th>\n",
       "      <th>learning_factor_high_learner</th>\n",
       "      <th>learning_factor_low_learner</th>\n",
       "      <th>teacher_learning_factor</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>parameter_combinations</th>\n",
       "      <th>max_loop_count</th>\n",
       "      <th>bec_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>common_knowledge</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.71</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.857353</td>\n",
       "      <td>0.055538</td>\n",
       "      <td>[0.6718128945981977, 0.777999972109676, 0.0555...</td>\n",
       "      <td>7</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>individual_knowledge_high</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.71</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.857353</td>\n",
       "      <td>0.055538</td>\n",
       "      <td>[0.6718128945981977, 0.777999972109676, 0.0555...</td>\n",
       "      <td>7</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>common_knowledge</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.674489</td>\n",
       "      <td>0.061297</td>\n",
       "      <td>[0.6146988332390071, 0.8455006172046735, 0.061...</td>\n",
       "      <td>6</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>individual_knowledge_high</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.74</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.674489</td>\n",
       "      <td>0.061297</td>\n",
       "      <td>[0.6146988332390071, 0.8455006172046735, 0.061...</td>\n",
       "      <td>8</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>common_knowledge</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.66</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.674489</td>\n",
       "      <td>0.061297</td>\n",
       "      <td>[0.6146988332390071, 0.8455006172046735, 0.061...</td>\n",
       "      <td>9</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>individual_knowledge_high</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.71</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.830671</td>\n",
       "      <td>0.063368</td>\n",
       "      <td>[0.6749770645683657, 0.8300326850743134, 0.063...</td>\n",
       "      <td>9</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>common_knowledge</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.73</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.600125</td>\n",
       "      <td>0.070761</td>\n",
       "      <td>[0.685646369817164, 0.7635257237653668, 0.0707...</td>\n",
       "      <td>9</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>individual_knowledge_high</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.77</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.600125</td>\n",
       "      <td>0.070761</td>\n",
       "      <td>[0.685646369817164, 0.7635257237653668, 0.0707...</td>\n",
       "      <td>7</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>common_knowledge</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.66</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.637519</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>[0.6770631817657633, 0.84972556577203, 0.00040...</td>\n",
       "      <td>9</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>individual_knowledge_high</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.71</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.637519</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>[0.6770631817657633, 0.84972556577203, 0.00040...</td>\n",
       "      <td>7</td>\n",
       "      <td>{'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 demo_strategy team_composition  study_id  run_id  \\\n",
       "0             common_knowledge        [0, 0, 0]         1       4   \n",
       "1    individual_knowledge_high        [0, 0, 0]         1       4   \n",
       "2             common_knowledge        [0, 0, 0]         3       1   \n",
       "3    individual_knowledge_high        [0, 0, 0]         3       1   \n",
       "4             common_knowledge        [0, 0, 0]         3       2   \n",
       "..                         ...              ...       ...     ...   \n",
       "287  individual_knowledge_high        [0, 0, 0]        26       1   \n",
       "288           common_knowledge        [0, 0, 0]        28       4   \n",
       "289  individual_knowledge_high        [0, 0, 0]        28       4   \n",
       "290           common_knowledge        [0, 0, 0]        30       3   \n",
       "291  individual_knowledge_high        [0, 0, 0]        30       3   \n",
       "\n",
       "     max_learning_factor  learning_factor_high_learner  \\\n",
       "0                   0.95                          0.71   \n",
       "1                   0.95                          0.71   \n",
       "2                   0.91                          0.70   \n",
       "3                   0.91                          0.74   \n",
       "4                   0.91                          0.66   \n",
       "..                   ...                           ...   \n",
       "287                 0.92                          0.71   \n",
       "288                 0.93                          0.73   \n",
       "289                 0.93                          0.77   \n",
       "290                 0.87                          0.66   \n",
       "291                 0.87                          0.71   \n",
       "\n",
       "    learning_factor_low_learner  teacher_learning_factor  learning_rate  \\\n",
       "0                            []                 0.857353       0.055538   \n",
       "1                            []                 0.857353       0.055538   \n",
       "2                            []                 0.674489       0.061297   \n",
       "3                            []                 0.674489       0.061297   \n",
       "4                            []                 0.674489       0.061297   \n",
       "..                          ...                      ...            ...   \n",
       "287                          []                 0.830671       0.063368   \n",
       "288                          []                 0.600125       0.070761   \n",
       "289                          []                 0.600125       0.070761   \n",
       "290                          []                 0.637519       0.000406   \n",
       "291                          []                 0.637519       0.000406   \n",
       "\n",
       "                                parameter_combinations  max_loop_count  \\\n",
       "0    [0.6718128945981977, 0.777999972109676, 0.0555...               7   \n",
       "1    [0.6718128945981977, 0.777999972109676, 0.0555...               7   \n",
       "2    [0.6146988332390071, 0.8455006172046735, 0.061...               6   \n",
       "3    [0.6146988332390071, 0.8455006172046735, 0.061...               8   \n",
       "4    [0.6146988332390071, 0.8455006172046735, 0.061...               9   \n",
       "..                                                 ...             ...   \n",
       "287  [0.6749770645683657, 0.8300326850743134, 0.063...               9   \n",
       "288  [0.685646369817164, 0.7635257237653668, 0.0707...               9   \n",
       "289  [0.685646369817164, 0.7635257237653668, 0.0707...               7   \n",
       "290  [0.6770631817657633, 0.84972556577203, 0.00040...               9   \n",
       "291  [0.6770631817657633, 0.84972556577203, 0.00040...               7   \n",
       "\n",
       "                                             bec_final  \n",
       "0    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "1    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "2    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "3    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "4    {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "..                                                 ...  \n",
       "287  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "288  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "289  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "290  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "291  {'p1': [1.0], 'p2': [1.0], 'p3': [1.0], 'commo...  \n",
       "\n",
       "[292 rows x 12 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "99d1070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pingouin import partial_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696e2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f6037c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cov() got an unexpected keyword argument 'numeric_only'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-94d4f8b79df8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcovars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvar2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpartial_corr_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial_corr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msensitivity_data_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_loop_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcovars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pearson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Print the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aug_taxi/lib/python3.9/site-packages/pingouin/correlation.py\u001b[0m in \u001b[0;36mpartial_corr\u001b[0;34m(data, x, y, covar, x_covar, y_covar, alternative, method)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna_option\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"keep\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m     \u001b[0mVi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Inverse covariance matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[0mVi_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cov() got an unexpected keyword argument 'numeric_only'"
     ]
    }
   ],
   "source": [
    "sensitivity_data_short = sensitivity_data[['max_loop_count', 'max_learning_factor', 'learning_factor_high_learner', 'teacher_learning_factor', 'learning_rate']]\n",
    "\n",
    "\n",
    "# Compute partial rank correlation coefficients\n",
    "partial_corr_results = {}\n",
    "var_list = ['max_learning_factor', 'learning_factor_high_learner', 'teacher_learning_factor', 'learning_rate']\n",
    "for var in var_list:\n",
    "    covars = [var2 for var2 in var_list if var2 != var]\n",
    "\n",
    "    partial_corr_results[var] = partial_corr(data=sensitivity_data_short, x='max_loop_count', y=var, covar=covars, method='pearson')\n",
    "\n",
    "# Print the results\n",
    "print(\"Partial Rank Correlation Coefficients:\")\n",
    "print(partial_corr_results)\n",
    "\n",
    "# fig, ax = plt.subplots(1, len(var_list), figsize=(10, 6))\n",
    "\n",
    "# for i, var in enumerate(var_list):\n",
    "#     ax[i].scatter(sensitivity_data_short[var], sensitivity_data_short['max_loop_count'])\n",
    "#     ax[i].set_xlabel(var)\n",
    "#     ax[i].set_ylabel('N_interactions')\n",
    "#     ax[i].set_title('Partial Rank Correlation Coefficient')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "correlation_matrix = sensitivity_data_short.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap between Input Parameters and Output')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
