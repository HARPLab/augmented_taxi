{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sureshkj/anaconda3/envs/aug_taxi/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:246: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n"
     ]
    }
   ],
   "source": [
    "# Analyze simulation data\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import teams.teams_helpers as teams_helpers\n",
    "import params_team as params\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "import os\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "import textwrap\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "import copy\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "import teams.teams_helpers as team_helpers\n",
    "import params_team as params\n",
    "# import simulation.sim_helpers as sim_helpers\n",
    "\n",
    "# from SALib.analyze import sobol\n",
    "from pingouin import partial_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28942961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_script(path, files, file_prefix_list, runs_to_exclude_list=[], runs_to_analyze_list = [], vars_filename_prefix = ''):\n",
    "    \n",
    "\n",
    "    ### load previously saved data\n",
    "    try:\n",
    "        with open (path + '/' + vars_filename_prefix + '_' + 'team_knowledge_level_long2.pickle', 'rb') as f:\n",
    "            team_knowledge_level_long = pickle.load(f)\n",
    "\n",
    "        with open (path + '/' + vars_filename_prefix + '_' + 'interaction_count2.pickle', 'rb') as f:\n",
    "            interaction_count = pickle.load(f)\n",
    "\n",
    "        with open (path + '/' + vars_filename_prefix + '_' + 'run_data2.pickle', 'rb') as f:\n",
    "            run_data = pickle.load(f)\n",
    "\n",
    "        with open (path + '/' + vars_filename_prefix + '_' + 'particles_prob2.pickle', 'rb') as f:\n",
    "            particles_prob = pickle.load(f)\n",
    "        \n",
    "        \n",
    "    except:\n",
    "    ### analyze data\n",
    "        team_unit_knowledge_level = pd.DataFrame()\n",
    "        team_BEC_knowledge_level_expected = pd.DataFrame()\n",
    "        team_BEC_knowledge_level = pd.DataFrame()\n",
    "        team_knowledge = pd.DataFrame()\n",
    "        learning_rate = pd.DataFrame()\n",
    "        likelihood_correct_response = pd.DataFrame()\n",
    "        particles_prob = pd.DataFrame()\n",
    "        learning_incomplete_runs = pd.DataFrame()\n",
    "\n",
    "        learning_complete_history = []\n",
    "        run_history = []\n",
    "\n",
    "        # intiialize unique simulation run id\n",
    "        run_id = 0\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            # check if file is a valid csv\n",
    "            for file_prefix in file_prefix_list:\n",
    "                if file_prefix in file and '.pickle' in file:\n",
    "                    run_file_flag = True\n",
    "                    break\n",
    "                else:\n",
    "                    run_file_flag = False\n",
    "\n",
    "            # check if file needs to be excluded\n",
    "            for runs_to_exclude in runs_to_exclude_list:\n",
    "                if runs_to_exclude in file:\n",
    "                    run_file_flag = False\n",
    "                    break\n",
    "\n",
    "            \n",
    "            if run_file_flag:\n",
    "\n",
    "                # sim_vars = pd.read_csv(path + '/' + file)\n",
    "\n",
    "                # print('bec_final: ', bec_final)\n",
    "                # bec_final = sim_vars['BEC_knowledge_level'][len(sim_vars)-1]\n",
    "                # BEC_team_knowledge_final= str_to_dict(bec_final, var_type = 'float')\n",
    "                # # check if learning was completed\n",
    "                # learning_complete = True\n",
    "                # for k_type, k_val in BEC_team_knowledge_final.items():\n",
    "                #     if k_val != 1:\n",
    "                #         learning_complete = False\n",
    "                #         break\n",
    "                \n",
    "                with open(path + '/' + file, 'rb') as f:\n",
    "                    sim_vars = pickle.load(f)\n",
    "                \n",
    "                bec_final = sim_vars['BEC_knowledge_level'][len(sim_vars)-1]\n",
    "                \n",
    "                # check if learning was completed\n",
    "                learning_complete = True\n",
    "                for k_type, k_val in bec_final.items():\n",
    "                    if k_val[0] != 1:\n",
    "                        learning_complete = False\n",
    "                        break\n",
    "\n",
    "\n",
    "                # print('learning_complete flag: ', learning_complete)\n",
    "                \n",
    "                if learning_complete:\n",
    "                # if True:\n",
    "\n",
    "                    print(colored('Reading file: ' + file + '. Run id: ' + str(run_id), 'blue' ))\n",
    "                    \n",
    "                    update_id = 1\n",
    "                    for i in range(len(sim_vars)):\n",
    "                        # unit knowledge level\n",
    "                        tk = sim_vars['unit_knowledge_level'][i]\n",
    "                        bec_k = sim_vars['BEC_knowledge_level'][i]\n",
    "                        bec_k_e = sim_vars['BEC_knowledge_level_expected'][i]\n",
    "                        tkc = sim_vars['team_knowledge'][i]\n",
    "                        # particles_prob = sim_vars['particles_prob_learner_demo'][i]\n",
    "                        # ilv = sim_vars['initial_likelihood_vars'][i]\n",
    "                        # lcr = sim_vars['likelihood_correct_response'][i]\n",
    "\n",
    "\n",
    "                        # if type(tk) == str and type(bec_k) == str and type(bec_k_e) == str and type(tkc) == str and type(ilv) == str and type(lcr) == str:\n",
    "                        # if type(tk) == str and type(bec_k) == str and type(bec_k_e) == str and type(tkc) == str:\n",
    "                        \n",
    "\n",
    "                        ## common vars\n",
    "                        test_constraints = sim_vars['test_constraints'][i]\n",
    "                        team_composition = str(sim_vars['team_composition'][i])\n",
    "                        print('team_composition: ', type(team_composition))\n",
    "                        print('test_constraints: ', test_constraints)\n",
    "\n",
    "\n",
    "                        ### Unit knowledge level\n",
    "                        # unit_knowledge_dict = str_to_dict(tk, var_type = 'float')\n",
    "\n",
    "                        \n",
    "                        unit_knowledge_dict = {}\n",
    "                        for key, val in tk.items():\n",
    "                            unit_knowledge_dict[key] = float(val[0])\n",
    "\n",
    "                        unit_knowledge_dict['run_no'] = run_id\n",
    "                        unit_knowledge_dict['demo_strategy'] = sim_vars['demo_strategy'][i]\n",
    "                        unit_knowledge_dict['loop_count'] = int(sim_vars['loop_count'][i])\n",
    "                        unit_knowledge_dict['knowledge_comp_id'] = int(sim_vars['knowledge_comp_id'][i])\n",
    "                        unit_knowledge_dict['file_name'] = file\n",
    "                        # print('unit_knowledge_dict: ', unit_knowledge_dict)\n",
    "                        team_unit_knowledge_level = team_unit_knowledge_level.append(unit_knowledge_dict, ignore_index=True, sort=False)\n",
    "\n",
    "                        ### BEC knowledge level \n",
    "                        # BEC_team_knowledge_dict = str_to_dict(bec_k, var_type = 'float')\n",
    "                        \n",
    "                        BEC_team_knowledge_dict = {}\n",
    "                        for key, val in bec_k.items():\n",
    "                            BEC_team_knowledge_dict[key] = float(val[0])\n",
    "\n",
    "                        BEC_team_knowledge_dict['run_no'] = run_id\n",
    "                        BEC_team_knowledge_dict['loop_count'] = int(sim_vars['loop_count'][i])\n",
    "                        BEC_team_knowledge_dict['demo_strategy'] = sim_vars['demo_strategy'][i]\n",
    "                        BEC_team_knowledge_dict['knowledge_comp_id'] = int(sim_vars['knowledge_comp_id'][i])\n",
    "                        BEC_team_knowledge_dict['team_composition'] = team_composition\n",
    "                        BEC_team_knowledge_dict['file_name'] = file\n",
    "\n",
    "                        # print('BEC_team_knowledge_dict: ', BEC_team_knowledge_dict)\n",
    "                        team_BEC_knowledge_level = team_BEC_knowledge_level.append(BEC_team_knowledge_dict, ignore_index=True, sort=False)\n",
    "                    \n",
    "                        ### expected BEC knowledge level\n",
    "                        # BEC_team_knowledge_dict_expected = str_to_dict(bec_k_e, var_type = 'float')\n",
    "\n",
    "                        BEC_team_knowledge_dict_expected = {}\n",
    "                        for key, val in bec_k_e.items():\n",
    "                            BEC_team_knowledge_dict_expected[key] = float(val[0])\n",
    "\n",
    "                        BEC_team_knowledge_dict_expected['run_no'] = run_id\n",
    "                        BEC_team_knowledge_dict_expected['loop_count'] = int(sim_vars['loop_count'][i])\n",
    "                        BEC_team_knowledge_dict_expected['demo_strategy'] = sim_vars['demo_strategy'][i]\n",
    "                        BEC_team_knowledge_dict_expected['knowledge_comp_id'] = int(sim_vars['knowledge_comp_id'][i])\n",
    "                        BEC_team_knowledge_dict_expected['file_name'] = file\n",
    "                        team_BEC_knowledge_level_expected = team_BEC_knowledge_level_expected.append(BEC_team_knowledge_dict_expected, ignore_index=True, sort=False)\n",
    "\n",
    "                        # # knowledge mix condition\n",
    "                        # learning_rate_dict = str_to_dict(ilv, var_type = 'array', splitter='),')\n",
    "                        # learning_rate_dict['run_no'] = run_id\n",
    "                        # learning_rate_dict['loop_count'] = int(sim_vars['loop_count'][i])\n",
    "                        # learning_rate_dict['demo_strategy'] = sim_vars['demo_strategy'][i]\n",
    "                        # learning_rate = learning_rate.append(learning_rate_dict, ignore_index=True)\n",
    "                        # # print('learning_rate_dict: ', learning_rate_dict)\n",
    "\n",
    "                        ### team knowledge constraints\n",
    "                        # team_knowledge_dict = str_to_dict(tkc, splitter = ', \\'')\n",
    "                        \n",
    "                        ## not working - to be fixed later; not being used currently\n",
    "                        # team_knowledge_dict = {}\n",
    "                        # for key, val in bec_k_e.items():\n",
    "                        #     print('key: ', key, ' val: ', val)\n",
    "                        #     team_knowledge_dict[key] = float(val[0])\n",
    "                        # team_knowledge_dict['run_no'] = run_id\n",
    "                        # team_knowledge_dict['demo_strategy'] = sim_vars['demo_strategy'][i]\n",
    "                        # team_knowledge_dict['loop_count'] = int(sim_vars['loop_count'][i])\n",
    "                        # team_knowledge_dict['file_name'] = file\n",
    "\n",
    "                        # print('team_knowledge_dict: ', team_knowledge_dict)\n",
    "                        # team_knowledge = team_knowledge.append(team_knowledge_dict, ignore_index=True)\n",
    "\n",
    "                        # # likelihood response\n",
    "                        # team_likelihood_correct_response_dict = {}\n",
    "                        # # print('lcr before: ', lcr)\n",
    "                        # lcr = lcr.strip('[]')\n",
    "                        # lcr = lcr.split(' ')\n",
    "                        # lcr = [i for i in lcr if i != '']\n",
    "                        # # print('lcr: ', lcr)\n",
    "                        # lcr_array = np.array(list(lcr), dtype=float)\n",
    "                        # # print('lcr_array: ', lcr_array)\n",
    "                        # team_likelihood_correct_response_dict['p1'] = lcr_array[0]\n",
    "                        # team_likelihood_correct_response_dict['p2'] = lcr_array[1]\n",
    "                        # team_likelihood_correct_response_dict['p3'] = lcr_array[2]\n",
    "                        # team_likelihood_correct_response_dict['common_knowledge'] = []\n",
    "                        # team_likelihood_correct_response_dict['joint_knowledge'] = []\n",
    "                        # team_likelihood_correct_response_dict['run_no'] = run_id\n",
    "                        # team_likelihood_correct_response_dict['demo_strategy'] = sim_vars['demo_strategy'][i]\n",
    "                        # team_likelihood_correct_response_dict['loop_count'] = int(sim_vars['loop_count'][i])\n",
    "                        # likelihood_correct_response = likelihood_correct_response.append(team_likelihood_correct_response_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "                        ### knowledge component and learning factor\n",
    "                        kc_variables = sim_vars['variable_filter'][i]\n",
    "                        print('kc_variables: ', type(kc_variables))\n",
    "                        if i==0:\n",
    "                            current_kc_variable = kc_variables\n",
    "                        if (current_kc_variable != kc_variables).any():\n",
    "                            update_id = 1  #reset update id for new KC\n",
    "                            current_kc_variable = kc_variables\n",
    "\n",
    "                        if (kc_variables == [[0, 1, 0]]).all():\n",
    "                            kc_id = 1\n",
    "                        elif (kc_variables == [[1, 0, 0]]).all():\n",
    "                            kc_id = 2\n",
    "                        elif (kc_variables == [[0, 0, 0]]).all():\n",
    "                            kc_id = 3\n",
    "                        else:\n",
    "                            print(colored('Unrecognized variable filter: ' + kc_variables, 'red'))\n",
    "\n",
    "                        # kc_variables = sim_vars['variable_filter'][i]\n",
    "                        # print('kc_variables: ', type(kc_variables))\n",
    "                        # if i==0:\n",
    "                        #     current_kc_variable = kc_variables\n",
    "                        # if (current_kc_variable != kc_variables):\n",
    "                        #     update_id = 1  #reset update id for new KC\n",
    "                        #     current_kc_variable = kc_variables\n",
    "\n",
    "                        # if (kc_variables == '[[0. 1. 0.]]'):\n",
    "                        #     kc_id = 1\n",
    "                        # elif (kc_variables == '[[1. 0. 0.]]'):\n",
    "                        #     kc_id = 2\n",
    "                        # elif (kc_variables == '[[0. 0. 0.]]'):\n",
    "                        #     kc_id = 3\n",
    "                        # else:\n",
    "                        #     print(colored('Unregognized variable filter: ' + kc_variables, 'red'))\n",
    "\n",
    "                        \n",
    "                        lf = sim_vars['team_learning_factor'][i]\n",
    "\n",
    "                        lf_array = lf\n",
    "\n",
    "                        # # print('lf: ', lf)\n",
    "                        # lf = lf.strip('[]')\n",
    "                        # lf = lf.split(' ')\n",
    "                        # lf = [i for i in lf if i != '']\n",
    "                        # # print('lcr: ', lcr)\n",
    "                        # lf_array = np.array(list(lf), dtype=float)\n",
    "                        # # print('lf_array: ', lf_array)\n",
    "\n",
    "                        \n",
    "\n",
    "                        # #### particles probability\n",
    "                        # if 'particles_prob_learner_demo' in sim_vars.columns:\n",
    "                        #     # team_particles_probability_dict = str_to_dict(sim_vars['particles_prob_learner_demo'][i], var_type=float)\n",
    "                        #     var_name = 'particles_prob_learner_demo'\n",
    "                        # elif 'particles_prob_learner_demos' in sim_vars.columns:\n",
    "                        #     # team_particles_probability_dict = str_to_dict(sim_vars['particles_prob_learner_demos'][i], var_type=float)\n",
    "                        #     var_name = 'particles_prob_learner_demos'\n",
    "                        # elif 'particles_prob_learner_before_test' in sim_vars.columns:\n",
    "                        #     # team_particles_probability_dict = str_to_dict(sim_vars['particles_prob_learner_before_test'][i], var_type=float)\n",
    "                        #     var_name = 'particles_prob_learner_before_test'\n",
    "\n",
    "                        for var_name in ['particles_prob_teacher_before_demo', 'particles_prob_learner_before_demo', 'particles_prob_teacher_after_demo', 'particles_prob_learner_after_demo', \\\n",
    "                                         'particles_prob_teacher_before_test', 'particles_prob_learner_before_test', 'particles_prob_teacher_after_test', 'particles_prob_teacher_after_feedback', 'particles_prob_learner_after_feedback']:\n",
    "\n",
    "                        \n",
    "                            # test_id = 1\n",
    "                            print('sim_vars[var_name][i]:', sim_vars[var_name][i])\n",
    "\n",
    "                            # ##  each test has probability separately\n",
    "                            # for prob_dict in sim_vars[var_name][i]:\n",
    "                            ## joint test has probability separately\n",
    "                            team_particles_probability_dict = {}\n",
    "                            for key, val in sim_vars[var_name][i].items():\n",
    "                                print('key: ', key, ' val: ', val)\n",
    "                                team_particles_probability_dict[key] = float(val)\n",
    "\n",
    "                            \n",
    "                            # for older trials; only one probability\n",
    "                            # if True:\n",
    "                            #     team_particles_probability_dict = str_to_dict(sim_vars[var_name][i], var_type=float)\n",
    "                            ###########\n",
    "\n",
    "                            print('team_particles_probability_dict: ', team_particles_probability_dict)\n",
    "\n",
    "                            for p_id, player in enumerate(team_particles_probability_dict):\n",
    "                                particles_probability_dict = {}\n",
    "                                particles_probability_dict['run_no'] = run_id\n",
    "                                particles_probability_dict['demo_strategy'] = sim_vars['demo_strategy'][i]\n",
    "                                particles_probability_dict['team_composition'] = team_composition\n",
    "                                particles_probability_dict['loop_count'] = int(sim_vars['loop_count'][i])\n",
    "                                # particles_probability_dict['test_id'] = test_id\n",
    "                                particles_probability_dict['test_constraints'] = test_constraints\n",
    "                                particles_probability_dict['update_id'] = update_id\n",
    "                                particles_probability_dict['update_type'] = var_name\n",
    "                                particles_probability_dict['kc_id'] = kc_variables\n",
    "                                particles_probability_dict['player_id'] = player\n",
    "                                particles_probability_dict['learning_factor'] = lf_array[p_id]\n",
    "                                particles_probability_dict['particles_prob'] = float(team_particles_probability_dict[player])\n",
    "                                particles_probability_dict['file_name'] = file\n",
    "\n",
    "                                particles_prob = particles_prob.append(particles_probability_dict, ignore_index=True, sort=False)\n",
    "\n",
    "                                # test_id += 1\n",
    "                        ################   \n",
    "                        \n",
    "                        update_id += 1\n",
    "\n",
    "                        # else:\n",
    "                        #     print(colored('Some non-string variables...','red'))\n",
    "\n",
    "\n",
    "                    # # BEC constraints\n",
    "                    # bec = sim_vars['min_BEC_constraints'][i]\n",
    "                    # bec = bec.replace('array', '')\n",
    "                    # bec = bec.replace('(', '')\n",
    "                    # bec = bec.replace(')', '')\n",
    "                    # bec = ast.literal_eval(bec)\n",
    "                    # bec_copy = []\n",
    "                    # for val in bec:\n",
    "                    #     bec_copy.extend(np.array([val]))\n",
    "                    # bec = bec_copy\n",
    "\n",
    "                    # plot knowledge constraints\n",
    "                    # teams_helpers.visualize_team_knowledge_constraints(bec, team_knowledge_dict, unit_knowledge_dict, BEC_team_knowledge_dict, params.mdp_class, weights=params.weights['val'])\n",
    "\n",
    "                        # team_BEC_knowledge_level = normalize_knowledge(team_BEC_knowledge_level)\n",
    "\n",
    "                else:\n",
    "                    print(colored('Learning incomplete for file: ' + file + '. Run id: ' + str(run_id), 'red' ))\n",
    "                    learning_incomplete_runs_dict = {}\n",
    "                    learning_incomplete_runs_dict['run_no'] = run_id\n",
    "                    learning_incomplete_runs_dict['file_name'] = file\n",
    "                    learning_incomplete_runs_dict['team_composition'] = str(sim_vars['team_composition'][0])\n",
    "                    learning_incomplete_runs_dict['demo_strategy'] = sim_vars['demo_strategy'][0]\n",
    "                    learning_incomplete_runs_dict['max_loop_count'] = sim_vars['loop_count'].iloc[-1]\n",
    "                    learning_incomplete_runs = learning_incomplete_runs.append(learning_incomplete_runs_dict, ignore_index=True, sort=False)\n",
    "                \n",
    "                ######\n",
    "                run_history.append(run_id)\n",
    "                learning_complete_history.append(learning_complete)\n",
    "                run_id += 1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        #### process team knowledge data\n",
    "        BEC_knowledge_level = []\n",
    "        BEC_knowledge_level_expected = []\n",
    "        knowledge_type = []\n",
    "        ind_knowledge_type = []\n",
    "        normalized_loop_count = []\n",
    "        lcr_var = []\n",
    "        team_mix_var = []\n",
    "        team_knowledge_level_min = team_BEC_knowledge_level.copy(deep=True)\n",
    "        print('team_knowledge_level_min columns: ', team_knowledge_level_min.columns)\n",
    "        team_knowledge_level_min = team_knowledge_level_min.drop(['p1', 'p2', 'p3', 'common_knowledge', 'joint_knowledge'], axis=1)\n",
    "\n",
    "\n",
    "        #### Long format data of team knowledge level\n",
    "        team_knowledge_level_long = pd.DataFrame()\n",
    "        for know_id in ['p1', 'p2', 'p3', 'common_knowledge', 'joint_knowledge']:\n",
    "            team_knowledge_level_long = pd.concat([team_knowledge_level_long, team_knowledge_level_min])\n",
    "            BEC_knowledge_level.extend(team_BEC_knowledge_level[know_id])\n",
    "            BEC_knowledge_level_expected.extend(team_BEC_knowledge_level_expected[know_id])\n",
    "            ind_knowledge_type.extend([know_id]*len(team_BEC_knowledge_level[know_id]))\n",
    "            # lcr_var.extend(likelihood_correct_response[know_id])\n",
    "            # team_mix_var.extend(learning_rate_index)\n",
    "            \n",
    "            if 'p' in know_id:\n",
    "                knowledge_type.extend(['individual']*len(team_BEC_knowledge_level[know_id]))\n",
    "            else:\n",
    "                knowledge_type.extend([know_id]*len(team_BEC_knowledge_level[know_id]))\n",
    "\n",
    "        normalized_loop_count = team_knowledge_level_long['loop_count'].copy(deep=True)\n",
    "        team_knowledge_level_long['BEC_knowledge_level'] = BEC_knowledge_level\n",
    "        team_knowledge_level_long['BEC_knowledge_level_expected'] = BEC_knowledge_level_expected\n",
    "        team_knowledge_level_long['knowledge_type'] = knowledge_type\n",
    "        team_knowledge_level_long['ind_knowledge_type'] = ind_knowledge_type\n",
    "        # team_knowledge_level_long['likelihood_correct_response'] = lcr_var\n",
    "        team_knowledge_level_long['normalized_loop_count'] = normalized_loop_count\n",
    "        # team_knowledge_level_long['team_mix'] = team_mix_var\n",
    "\n",
    "        \n",
    "        team_knowledge_level_long.to_csv(path + '/' + vars_filename_prefix + '_' + 'team_knowledge_level_long.csv')\n",
    "        with open (path + '/' + vars_filename_prefix + '_' + 'team_knowledge_level_long.pickle', 'wb') as f:\n",
    "            pickle.dump(team_knowledge_level_long, f)\n",
    "        team_knowledge_level_long.describe(include='all').to_csv(path + '/' + vars_filename_prefix + '_' + 'descriptives.csv')\n",
    "        # team_knowledge_level_long = pd.read_csv('models/augmented_taxi2/team_knowledge_level_long.csv')\n",
    "\n",
    "        \n",
    "        ############## concept-wise interaction count\n",
    "        concept_ids = team_BEC_knowledge_level['knowledge_comp_id'].unique()\n",
    "        unique_ids = team_knowledge_level_long['run_no'].unique()\n",
    "        interaction_count = pd.DataFrame()\n",
    "\n",
    "        team_BEC_knowledge_level.to_csv(path + '/' + vars_filename_prefix + '_' + 'team_BEC_knowledge_level.csv')\n",
    "        print(team_BEC_knowledge_level)\n",
    "\n",
    "        # for id in unique_ids:\n",
    "        for id in range(len(run_history)):\n",
    "            interaction_count_dict = {}\n",
    "            print('Run: ', run_history[id])\n",
    "            for c_id in range(len(concept_ids)):\n",
    "                interaction_count_dict['run_no'] = run_history[id]\n",
    "                interaction_count_dict['learning_complete_flag'] = learning_complete_history[id]\n",
    "                if learning_complete_history[id]:\n",
    "                    interaction_count_dict['demo_strategy'] = ''.join(str(element) for element in team_BEC_knowledge_level[team_BEC_knowledge_level['run_no'] == run_history[id]]['demo_strategy'].iloc[0])\n",
    "                    interaction_count_dict['team_composition'] = ''.join(str(element) for element in team_BEC_knowledge_level[team_BEC_knowledge_level['run_no'] == run_history[id]]['team_composition'].iloc[0])\n",
    "\n",
    "                    run_team_knowledge_data = team_BEC_knowledge_level[team_BEC_knowledge_level['run_no'] == run_history[id]]\n",
    "                    \n",
    "                    if concept_ids[c_id] <= run_team_knowledge_data['knowledge_comp_id'].iloc[-1]:\n",
    "                        loop_id_list = []\n",
    "                        for idx, row in team_BEC_knowledge_level.iterrows():\n",
    "                            # print('row[run_no]: ', row['run_no'], ' run_history[id]: ', run_history[id], ' row[knowledge_comp_id]: ', row['knowledge_comp_id'], ' concept_ids[c_id]: ', concept_ids[c_id])\n",
    "                            if row['run_no'] == run_history[id] and row['knowledge_comp_id'] == concept_ids[c_id]:\n",
    "                                loop_id_list.append(row['loop_count'])\n",
    "                        \n",
    "                        max_loop_count = max(loop_id_list)-min(loop_id_list)+1\n",
    "                    \n",
    "                else:\n",
    "                    interaction_count_dict['demo_strategy'] = ''\n",
    "                    interaction_count_dict['team_composition'] = ''\n",
    "                    max_loop_count = 0\n",
    "\n",
    "\n",
    "                interaction_count_dict['Int_end_id_concept_'+str(concept_ids[c_id])] = max(loop_id_list)\n",
    "                interaction_count_dict['N_int_concept_'+str(concept_ids[c_id])] = max_loop_count    \n",
    "\n",
    "            interaction_count = interaction_count.append(interaction_count_dict, ignore_index=True)\n",
    "\n",
    "        print('interaction_count: ', interaction_count)\n",
    "        interaction_count.to_csv(path + '/' + vars_filename_prefix + '_' + 'interaction_count.csv')\n",
    "        with open (path + '/' + vars_filename_prefix + '_' + 'interaction_count.pickle', 'wb') as f:\n",
    "            pickle.dump(interaction_count, f)\n",
    "        ######################################################################\n",
    "        \n",
    "\n",
    "        ## run-wise data\n",
    "        # unique_ids = team_knowledge_level_long['run_no'].unique()\n",
    "        \n",
    "        run_data = pd.DataFrame()\n",
    "        # for id in unique_ids:\n",
    "        for id in range(len(run_history)):\n",
    "            run_data_dict = {}\n",
    "            run_data_dict['run_no'] = run_history[id]\n",
    "            run_data_dict['learning_complete_flag'] = learning_complete_history[id]\n",
    "            if learning_complete_history[id]:\n",
    "                run_data_dict['demo_strategy'] = ''.join(str(element) for element in team_BEC_knowledge_level[team_BEC_knowledge_level['run_no'] == id]['demo_strategy'].iloc[0])\n",
    "                run_data_dict['max_loop_count'] = int(team_BEC_knowledge_level[team_BEC_knowledge_level['run_no'] == id]['loop_count'].iloc[-1])\n",
    "                run_data_dict['team_composition'] = ''.join(str(element) for element in team_knowledge_level_long[team_knowledge_level_long['run_no'] == id]['team_composition'].iloc[0])\n",
    "                run_data_dict['file_name'] = ''.join(str(element) for element in team_knowledge_level_long[team_knowledge_level_long['run_no'] == id]['file_name'].iloc[0])\n",
    "            else:\n",
    "                run_data_dict['demo_strategy'] = ''\n",
    "                run_data_dict['max_loop_count'] = 0\n",
    "                run_data_dict['team_composition'] = ''\n",
    "                run_data_dict['file_name'] = ''\n",
    "\n",
    "            run_data = run_data.append(run_data_dict, ignore_index=True)\n",
    "\n",
    "        run_data.to_csv(path + '/' + vars_filename_prefix + '_' + 'run_data.csv')\n",
    "        with open (path + '/' + vars_filename_prefix + '_' + 'run_data.pickle', 'wb') as f:\n",
    "            pickle.dump(run_data, f)\n",
    "\n",
    "        print(colored('Number of runs processed: ' + str(len(run_data)), 'red'))\n",
    "\n",
    "        # run_data = pd.read_csv('models/augmented_taxi2/run_data.csv')\n",
    "\n",
    "        print('Incomplete runs: ', learning_incomplete_runs)\n",
    "        learning_incomplete_runs.to_csv(path + '/' + vars_filename_prefix + '_' + 'learning_incomplete_runs.csv')\n",
    "        with open (path + '/' + vars_filename_prefix + '_' + 'learning_incomplete_runs.pickle', 'wb') as f:\n",
    "            pickle.dump(learning_incomplete_runs, f)\n",
    "        \n",
    "        ###\n",
    "        particles_prob.to_csv(path + '/' + vars_filename_prefix + '_' + 'particles_prob.csv')\n",
    "        with open (path + '/' + vars_filename_prefix + '_' + 'particles_prob.pickle', 'wb') as f:\n",
    "            pickle.dump(particles_prob, f)\n",
    "\n",
    "        ## normalize loop count\n",
    "\n",
    "        # for id in unique_ids:\n",
    "        #     idx = team_knowledge_level_long[(team_knowledge_level_long['run_no'] == id)].index\n",
    "        #     max_loop_count = np.max(team_knowledge_level_long.loc[idx, 'loop_count'])\n",
    "        #     team_knowledge_level_long.loc[idx, 'normalized_loop_count'] = team_knowledge_level_long.loc[idx, 'loop_count']/max_loop_count\n",
    "\n",
    "\n",
    "        # print(team_knowledge_level_long)\n",
    "        # print(knowledge_type)\n",
    "\n",
    "    ###################################################################################################################\n",
    "\n",
    "    ##############################################   Plots    ##########################################################\n",
    "\n",
    "\n",
    "\n",
    "    # for know_id in ['p1', 'p2', 'p3', 'common_knowledge', 'joint_knowledge']:\n",
    "        # sns.lineplot(data = team_BEC_knowledge_level, x = 'loop_count', y = know_id, hue = 'demo_strategy').set(title='Knowledge level for ' + know_id)\n",
    "        # plt.show()\n",
    "        # plt.savefig('models/augmented_taxi2/BEC_knowledge_level_' + know_id + '.png')\n",
    "        # plt.close()\n",
    "    \n",
    "    \n",
    "    # f, ax = plt.subplots(nrows=2,ncols=3, sharex=True, sharey=True, figsize=(10,6))\n",
    "    # plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    # row_id = 0\n",
    "    # col_id = 0\n",
    "    # for team_mix_cond in [[0, 1, 1]]:\n",
    "    #     col_id = 0\n",
    "    #     for know_id in ['individual', 'common_knowledge', 'joint_knowledge']:    \n",
    "    #         plot_data = team_knowledge_level_long[(team_knowledge_level_long['knowledge_type']==know_id) & (team_knowledge_level_long['team_composition']==str(team_mix_cond))]\n",
    "    #         # plot_data.to_csv('models/augmented_taxi2/plot_data_' + know_id + '_' + str(team_mix_cond) + '.csv')\n",
    "    #         print('plot_data: ', type(plot_data))\n",
    "    #         sns.lineplot(plot_data, x = 'loop_count', y = 'BEC_knowledge_level', hue = 'demo_strategy', ax=ax[row_id, col_id], errorbar=('se', 1), err_style=\"band\", hue_order = ['individual_knowledge_low','individual_knowledge_high','common_knowledge','joint_knowledge'], legend=False).set(title='Knowledge level for ' + know_id + ' with a team mix: ' + str(team_mix_cond))\n",
    "    #         col_id += 1 \n",
    "    #     row_id += 1\n",
    "\n",
    "\n",
    "    ##########\n",
    "    \n",
    "    ## Choose conditions to plot\n",
    "    know_list_full = ['individual', 'common_knowledge', 'joint_knowledge']\n",
    "    team_mix_full = [[0,0,0], [0,0,2], [0,2,2], [2,2,2]]\n",
    "    demo_list = ['individual_knowledge_low','individual_knowledge_high','common_knowledge','joint_knowledge']\n",
    "    \n",
    "\n",
    "    know_list = know_list_full[0:]\n",
    "    team_mix = team_mix_full[0:]\n",
    "\n",
    "    # Plot knowledge level for each combination of team composition and knowledge type\n",
    "    col_id = 0\n",
    "    for team_mix_cond in team_mix:\n",
    "        col_id = 0\n",
    "        f, ax = plt.subplots(ncols=3, sharex=True, sharey=True, figsize=(10,6))\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "        for know_id in know_list:    \n",
    "            plot_data = team_knowledge_level_long[(team_knowledge_level_long['knowledge_type']==know_id) & (team_knowledge_level_long['team_composition']==str(team_mix_cond))]\n",
    "            \n",
    "            # plot_data.to_csv('models/augmented_taxi2/plot_data_' + know_id + '_' + str(team_mix_cond) + '.csv')\n",
    "            # print('Plotting  ', 'row_id: ', row_id, ' col_id: ', col_id, ' know_id: ', know_id, ' team_mix_cond: ', team_mix_cond)\n",
    "            sns.lineplot(plot_data, x = 'loop_count', y = 'BEC_knowledge_level', hue = 'demo_strategy', ax=ax[col_id], errorbar=('se', 1), err_style=\"band\", hue_order = ['individual_knowledge_low','individual_knowledge_high','common_knowledge','joint_knowledge']).set(title='Knowledge level for ' + know_id + ' with a team mix: ' + str(team_mix_cond))\n",
    "            # sns.lineplot(plot_data, x = 'loop_count', y = 'BEC_knowledge_level', hue = 'demo_strategy', ax=ax[row_id, col_id], errorbar=('se', 1), err_style=\"band\", hue_order = ['individual_knowledge_low','individual_knowledge_high','common_knowledge','joint_knowledge'], legend=False).set(title='Knowledge level for ' + know_id + ' with a team mix: ' + str(team_mix_cond))\n",
    "\n",
    "            col_id += 1 \n",
    "        \n",
    "    # plt.show()\n",
    "            \n",
    "    # plt.savefig('models/augmented_taxi2/BEC_knowledge_level_' + know_id + '.png')\n",
    "    # plt.close()\n",
    "    ########\n",
    "        \n",
    "    ## plot knowledge level for all conditions\n",
    "    f, ax = plt.subplots(ncols=3, sharex=True, sharey=True, figsize=(10,6))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    col_id = 0\n",
    "    for know_id in know_list:  \n",
    "        plot_data = team_knowledge_level_long[(team_knowledge_level_long['knowledge_type']==know_id) ]\n",
    "        sns.lineplot(plot_data, x = 'loop_count', y = 'BEC_knowledge_level', ax=ax[col_id], errorbar=('se', 1), err_style=\"band\").set(title='Knowledge level for a team mix: ' + str(team_mix_cond))\n",
    "        # sns.lineplot(plot_data, x = 'loop_count', y = 'BEC_knowledge_level', hue = 'demo_strategy', ax=ax[row_id, col_id], errorbar=('se', 1), err_style=\"band\", hue_order = ['individual_knowledge_low','individual_knowledge_high','common_knowledge','joint_knowledge'], legend=False).set(title='Knowledge level for ' + know_id + ' with a team mix: ' + str(team_mix_cond))\n",
    "        col_id += 1 \n",
    "    # plt.show()\n",
    "    ########\n",
    "\n",
    "    ## plot knowledge level for all team composition\n",
    "    for team_mix_cond in team_mix:\n",
    "        f, ax = plt.subplots(ncols=3, sharex=True, sharey=True, figsize=(10,6))\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "        col_id = 0\n",
    "        for know_id in know_list:  \n",
    "            plot_data = team_knowledge_level_long[(team_knowledge_level_long['knowledge_type']==know_id) &(team_knowledge_level_long['team_composition']==str(team_mix_cond))]\n",
    "            sns.lineplot(plot_data, x = 'loop_count', y = 'BEC_knowledge_level', ax=ax[col_id], errorbar=('se', 1), err_style=\"band\").set(title='Knowledge level for a team mix: ' + str(team_mix_cond))\n",
    "            # sns.lineplot(plot_data, x = 'loop_count', y = 'BEC_knowledge_level', hue = 'demo_strategy', ax=ax[row_id, col_id], errorbar=('se', 1), err_style=\"band\", hue_order = ['individual_knowledge_low','individual_knowledge_high','common_knowledge','joint_knowledge'], legend=False).set(title='Knowledge level for ' + know_id + ' with a team mix: ' + str(team_mix_cond))\n",
    "            col_id += 1 \n",
    "        # plt.show()\n",
    "\n",
    "    # plot knowledge for demo strategy\n",
    "    for demo_id in demo_list:\n",
    "        f, ax = plt.subplots(ncols=3, sharex=True, sharey=True, figsize=(10,6))\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "        col_id = 0\n",
    "        for know_id in know_list: \n",
    "            plot_data = team_knowledge_level_long[(team_knowledge_level_long['knowledge_type']==know_id) &(team_knowledge_level_long['demo_strategy']==str(demo_id))]\n",
    "            sns.lineplot(plot_data, x = 'loop_count', y = 'BEC_knowledge_level', ax=ax[col_id], errorbar=('se', 1), err_style=\"band\").set(title='Knowledge level for demo strategy: ' + str(demo_id))\n",
    "            # sns.lineplot(plot_data, x = 'loop_count', y = 'BEC_knowledge_level', hue = 'demo_strategy', ax=ax[row_id, col_id], errorbar=('se', 1), err_style=\"band\", hue_order = ['individual_knowledge_low','individual_knowledge_high','common_knowledge','joint_knowledge'], legend=False).set(title='Knowledge level for ' + know_id + ' with a team mix: ' + str(team_mix_cond))\n",
    "            col_id += 1 \n",
    "        # plt.show()\n",
    "    \n",
    "    ########################\n",
    "\n",
    "    ## Another knowledge level plot\n",
    "    # sea = sns.FacetGrid(team_knowledge_level_long, row = \"knowledge_type\")\n",
    "    # sea.map(sns.lineplot, \"loop_count\", \"BEC_knowledge_level\", \"demo_strategy\")\n",
    "    # sea.set_axis_labels(\"Number of interaction sets/lessons\", \"Knowledge level\")\n",
    "    # sea.add_legend()\n",
    "\n",
    "    # sea = sns.FacetGrid(team_knowledge_level_long, row = \"knowledge_type\")\n",
    "    # sea.map(sns.lineplot, \"normalized_loop_count\", \"BEC_knowledge_level\", \"demo_strategy\")\n",
    "    # sea.set_axis_labels(\"Number of interaction sets/lessons\", \"Knowledge level\")\n",
    "    # sea.add_legend()\n",
    "    # plt.show(block='False')\n",
    "\n",
    "    # # for each run\n",
    "    # for id in unique_ids:\n",
    "    #     idx = team_knowledge_level_long[(team_knowledge_level_long['run_no'] == id)].index\n",
    "    #     sea = sns.FacetGrid(team_knowledge_level_long.loc[idx,:], row = \"knowledge_type\")\n",
    "    #     sea.map(sns.lineplot, \"normalized_loop_count\", \"BEC_knowledge_level\", \"demo_strategy\")\n",
    "    #     sea.set_axis_labels(\"Number of interaction sets/lessons\", \"Knowledge level\")\n",
    "    #     sea.add_legend()\n",
    "    #     plt.show(block='False')\n",
    "\n",
    "\n",
    "    # ## plot team BEC knowledge level\n",
    "    # for id in unique_ids:\n",
    "    #     idx = team_BEC_knowledge_level[(team_BEC_knowledge_level['run_no'] == id)].index\n",
    "\n",
    "    #     # print(team_BEC_knowledge_level.columns)\n",
    "\n",
    "    #     fig, axs = plt.subplots(2, 3, figsize=(9, 5), layout='constrained',\n",
    "    #                     sharex=True, sharey=True)\n",
    "    #     fig.suptitle('Run: ' + str(id))\n",
    "    #     for nn, ax in enumerate(axs.flat):\n",
    "    #         column_name = team_BEC_knowledge_level.columns[nn]\n",
    "    #         y = team_BEC_knowledge_level.loc[idx,column_name]\n",
    "    #         line, = ax.plot(team_BEC_knowledge_level.loc[idx, 'loop_count'], y, lw=2.5)\n",
    "    #         ax.set_title(column_name, fontsize='small', loc='center')\n",
    "\n",
    "\n",
    "    #         # plot verticle lines for visualizng end of concepts\n",
    "    #         for kc_id in team_BEC_knowledge_level['knowledge_comp_id'].unique():\n",
    "    #             max_idx = team_BEC_knowledge_level[(team_BEC_knowledge_level['knowledge_comp_id'] == kc_id) & (team_BEC_knowledge_level['run_no'] == id)].index.max()\n",
    "    #             # print('kc_id:', kc_id, ' max_idx: ', max_idx)\n",
    "    #             if not math.isnan(max_idx):\n",
    "    #                 ax.axvline(team_BEC_knowledge_level.loc[max_idx, 'loop_count'], color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "    #         if nn == 4:\n",
    "    #             break\n",
    "\n",
    "    #     fig.supxlabel('Interaction Number')\n",
    "    #     fig.supylabel('BEC Knowledge Level')\n",
    "\n",
    "\n",
    "    ## Plot Unit knowledge level\n",
    "    # fig2, axs2 = plt.subplots(2, 3, figsize=(9, 5), layout='constrained',\n",
    "    #                 sharex=True, sharey=True)\n",
    "    # for nn, ax in enumerate(axs2.flat):\n",
    "    #     column_name = team_unit_knowledge_level.columns[nn]\n",
    "    #     y = team_unit_knowledge_level.loc[idx,column_name]\n",
    "    #     line, = ax.plot(team_unit_knowledge_level.loc[idx, 'loop_count'], y, lw=2.5)\n",
    "    #     ax.set_title(column_name, fontsize='small', loc='center')\n",
    "\n",
    "    #     # plot verticle lines for visualizng end of concepts\n",
    "    #     for kc_id in team_BEC_knowledge_level['knowledge_comp_id'].unique():\n",
    "    #         max_idx = team_BEC_knowledge_level[(team_BEC_knowledge_level['knowledge_comp_id'] == kc_id) & (team_BEC_knowledge_level['run_no'] == id)].index.max()\n",
    "    #         ax.axvline(team_BEC_knowledge_level.loc[max_idx, 'loop_count'], color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "    #     if nn == 4:\n",
    "    #         break\n",
    "\n",
    "    # fig2.supxlabel('Interaction Number')\n",
    "    # fig2.supylabel('Unit Knowledge Level')\n",
    "\n",
    "        \n",
    "    #############\n",
    "    # plot interaction count for concepts\n",
    "            \n",
    "    # histogram\n",
    "    f, ax_sbc = plt.subplots(ncols=1)\n",
    "    sns.histplot(data = run_data, x='max_loop_count', hue='learning_complete_flag', ax=ax_sbc).set(title='Distribution of maximum interactions for learning')\n",
    "\n",
    "    # f, ax_sbc = plt.subplots(ncols=1)\n",
    "    # sns.barplot(data = interaction_count, x = 'knowledge_comp_id', y = 'max_loop_count', hue = 'demo_strategy', ax=ax_sbc, errorbar=('se',1)).set(title='Interaction count for concepts')\n",
    "\n",
    "    # column_data_types = run_data.dtypes\n",
    "\n",
    "    # print('run_data column data type:', column_data_types)\n",
    "\n",
    "    # plot interaction count overall\n",
    "    f, ax_c = plt.subplots(ncols=1)\n",
    "    sns.barplot(data = run_data, x = 'demo_strategy', y = 'max_loop_count', hue = 'team_composition', ax=ax_c, errorbar=('se',1)).set(title='Max number of interactions')\n",
    "\n",
    "\n",
    "    f2, ax_2 = plt.subplots(ncols=2)\n",
    "    sns.barplot(data = run_data, x = 'demo_strategy', y = 'max_loop_count', ax=ax_2[0], errorbar=('se',1)).set(title='Max number of interactions vs. Demo Strategy')\n",
    "    sns.barplot(data = run_data, x = 'team_composition', y = 'max_loop_count', ax=ax_2[1], errorbar=('se',1)).set(title='Max number of interactions vs. Team composition')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #############\n",
    "\n",
    "    ### plot probability of particles in the correct side of test\n",
    "    # simulation run conditions\n",
    "    dem_strategy_list = ['individual_knowledge_low', 'individual_knowledge_high', 'common_knowledge', 'joint_knowledge']\n",
    "    kc_id_list = ['[[0. 1. 0.]]', '[[1. 0. 0.]]', '[[0. 0. 0.]]']\n",
    "    team_mix_list = ['[0, 0, 0]', '[0, 0, 2]', '[0, 2, 2]', '[2, 2, 2]']\n",
    "\n",
    "    # print('particles_prob: ', particles_prob)\n",
    "    \n",
    "    \n",
    "\n",
    "    # ## plotting seprately for each team condition and demo strategy\n",
    "    # for team_composition in team_mix_list:\n",
    "    #     for dem_strategy in dem_strategy_list:\n",
    "\n",
    "    #         run_data = particles_prob[(particles_prob['team_composition']==str(team_composition)) & (particles_prob['demo_strategy']==dem_strategy)]\n",
    "\n",
    "    #         if len(run_data) > 0:\n",
    "    #             f3, ax3 = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(10,6))\n",
    "    #             plt.subplots_adjust(wspace=0.1, hspace=0.5)  \n",
    "            \n",
    "    #             kc_no = 0\n",
    "    #             for kc_id in kc_id_list:\n",
    "    #                 print('team_composition: ', team_composition, ' dem_strategy: ', dem_strategy)\n",
    "\n",
    "    #                 plot_data = particles_prob[(particles_prob['team_composition']==str(team_composition)) & (particles_prob['demo_strategy']==dem_strategy) & (particles_prob['kc_id']==kc_id)]\n",
    "    #                 if len(plot_data) > 0:\n",
    "    #                     print('plot_data: ', plot_data)\n",
    "    #                     plot_title = 'Learning factor vs. particles_probability for kc ' +  kc_id + ', team mix: ' + str(team_composition) + ' and a demo strategy: ' + dem_strategy\n",
    "    #                     wrapped_title = \"\\n\".join(textwrap.wrap(plot_title, 40))\n",
    "    #                     sns.lineplot(plot_data, x = 'learning_factor', y = 'particles_prob', ax=ax3[0, kc_no], legend=True).set(title=wrapped_title)\n",
    "    #                     plot_title = 'Updatewise Learning factor vs. particles_probability for kc ' +  kc_id + ', team mix: ' + str(team_composition) + ' and a demo strategy: ' + dem_strategy\n",
    "    #                     wrapped_title = \"\\n\".join(textwrap.wrap(plot_title, 40))\n",
    "    #                     sns.lineplot(plot_data, x = 'learning_factor', y = 'particles_prob', hue = 'update_id', ax=ax3[1, kc_no], legend=True).set(title=wrapped_title)\n",
    "    #                     kc_no += 1\n",
    "        \n",
    "    #             plt.show()\n",
    "    ########\n",
    "\n",
    "\n",
    "    # ## plotting for all experiment conditions\n",
    "\n",
    "    f3, ax3 = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(10,6))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.5)  \n",
    "\n",
    "    kc_no = 0\n",
    "    for kc_id in kc_id_list:\n",
    "        print('kc_id: ', kc_id, 'particles_prob[kc_id]: ', type(particles_prob['kc_id']))\n",
    "        plot_data_idx = []\n",
    "        for idx, row in particles_prob.iterrows():\n",
    "            if (row['kc_id'] == kc_id).all():\n",
    "                plot_data_idx.append(idx)\n",
    "        plot_data = particles_prob.loc[plot_data_idx]\n",
    "        # plot_data = particles_prob[(particles_prob['kc_id'][0]==kc_id).all()]\n",
    "        if len(plot_data) > 0:\n",
    "            print('plot_data: ', plot_data)\n",
    "            plot_title = 'Learning factor vs. particles_probability for kc ' +  kc_id \n",
    "            wrapped_title = \"\\n\".join(textwrap.wrap(plot_title, 40))\n",
    "            sns.lineplot(plot_data, x = 'learning_factor', y = 'particles_prob', ax=ax3[0, kc_no], legend=True).set(title=wrapped_title)\n",
    "            plot_title = 'Updatewise Learning factor vs. particles_probability for kc ' +  kc_id\n",
    "            wrapped_title = \"\\n\".join(textwrap.wrap(plot_title, 40))\n",
    "            sns.lineplot(plot_data, x = 'learning_factor', y = 'particles_prob', hue = 'update_id', ax=ax3[1, kc_no], legend=True).set(title=wrapped_title)\n",
    "            kc_no += 1\n",
    "\n",
    "    plt.show()\n",
    "    #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979a7508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debug_trials_02_05_no_noise_w_feedback_incorrect_study_1_run_103']\n",
      "['no_review']\n",
      "Reading file: debug_trials_02_05_no_noise_w_feedback_incorrect_study_1_run_103.pickle. Run id: 0\n",
      "team_composition:  <class 'str'>\n",
      "test_constraints:  [[array([[ 1,  0, -4]]), array([[-1,  0,  2]])]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6d208963dddd>\u001b[0m in \u001b[0;36mrun_analysis_script\u001b[0;34m(path, files, file_prefix_list, runs_to_exclude_list, runs_to_analyze_list, vars_filename_prefix)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvars_filename_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'team_knowledge_level_long2.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mteam_knowledge_level_long\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/augmented_taxi2/analysis_team_knowledge_level_long2.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fc3d962421eb>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mrun_analysis_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_prefix_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruns_to_exclude_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruns_to_exclude_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars_filename_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars_filename_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6d208963dddd>\u001b[0m in \u001b[0;36mrun_analysis_script\u001b[0;34m(path, files, file_prefix_list, runs_to_exclude_list, runs_to_analyze_list, vars_filename_prefix)\u001b[0m\n\u001b[1;32m    122\u001b[0m                         \u001b[0munit_knowledge_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                         \u001b[0;31m# print('unit_knowledge_dict: ', unit_knowledge_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         \u001b[0mteam_unit_knowledge_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteam_unit_knowledge_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit_knowledge_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;31m### BEC knowledge level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aug_taxi/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6294\u001b[0m         ):\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6296\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6298\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# process team knowledge data\n",
    "path = 'models/augmented_taxi2'\n",
    "# path = 'data/simulation/sim_experiments/w_feedback'\n",
    "files = os.listdir(path)\n",
    "\n",
    "# all_file_prefix_list = ['debug_data_debug_trials_01_22_no_noise_w_feedback_study_1']\n",
    "# all_runs_to_exclude_list = [[3, 12, 24, 7], [1,4,6,8], [], [1,3, 11, 12, 16, 18], [17, 21, 35], [], [], [], \\\n",
    "#                             [], [], [], [], [], [], []]\n",
    "all_runs_to_exclude_list = []\n",
    "\n",
    "# sets_to_consider = [14]\n",
    "# file_prefix_list = [all_file_prefix_list[i] for i in sets_to_consider]\n",
    "# runs_to_exclude_list = [all_runs_to_exclude_list[i] for i in sets_to_consider]\n",
    "\n",
    "# file_prefix_list = ['trials_12_29_w_updated', 'trials_12_30_w_updated', 'trials_12_31_w_updated', 'trials_01_01_w_updated', \n",
    "#                     'trials_01_02_w_updated', 'trials_01_03_w_updated', 'trials_01_04_w_updated']\n",
    "\n",
    "file_prefix_list = ['debug_trials_02_05_no_noise_w_feedback_incorrect_study_1_run_103']\n",
    "\n",
    "# runs_to_exclude_list = ['unfinished', 'trials_01_01_w_updated_noise_57'] \n",
    "runs_to_exclude_list = ['no_review']\n",
    "# trials_01_01_w_updated_noise_57.csv - outlier, N = 48 trials\n",
    "\n",
    "vars_filename_prefix = 'analysis'\n",
    "\n",
    "print(file_prefix_list)\n",
    "print(runs_to_exclude_list)\n",
    "\n",
    "\n",
    "run_analysis_script(path, files, file_prefix_list, runs_to_exclude_list = runs_to_exclude_list, vars_filename_prefix = vars_filename_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
