{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c21aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sureshkj/anaconda3/envs/aug_taxi/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:246: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: OpenAI gym not installed.\n"
     ]
    }
   ],
   "source": [
    "# Analyze simulation data\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import teams.teams_helpers as teams_helpers\n",
    "import params_team as params\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "import os\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "import textwrap\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "import copy\n",
    "from collections import Counter\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "import teams.teams_helpers as team_helpers\n",
    "import params_team as params\n",
    "import policy_summarization.BEC_helpers as BEC_helpers\n",
    "# import simulation.sim_helpers as sim_helpers\n",
    "from simple_rl.planning import ValueIteration\n",
    "\n",
    "\n",
    "# from SALib.analyze import sobol\n",
    "from pingouin import partial_corr\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import teams.teams_helpers as teams_helpers\n",
    "import params_team as params\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "import os\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "import textwrap\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "import copy\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# Set the seaborn color palette to colorblind\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Get the colorblind palette with the desired number of colors\n",
    "colorblind_palette = sns.color_palette(\"colorblind\", 6)\n",
    "\n",
    "# Set the default color cycle for Matplotlib using rcParams\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=colorblind_palette)\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "\n",
    "import teams.teams_helpers as team_helpers\n",
    "import params_team as params\n",
    "# import simulation.sim_helpers as sim_helpers\n",
    "from teams import particle_filter_team as pf_team\n",
    "import policy_summarization.BEC_visualization as BEC_viz\n",
    "import policy_summarization.BEC_helpers as BEC_helpers\n",
    "\n",
    "import sage.geometry.polyhedron.base as Polyhedron\n",
    "import sage.all\n",
    "from statsmodels.graphics.factorplots import interaction_plot\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "\n",
    "# from SALib.analyze import sobol\n",
    "from pingouin import partial_corr\n",
    "import teams.utils_teams as utils_teams\n",
    "import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from sklearn import metrics\n",
    "from scipy.stats import bartlett, levene\n",
    "\n",
    "from policy_summarization import computational_geometry as cg\n",
    "\n",
    "import ast\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import dill as pickle\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073e1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/skateboard2/team_BEC_constraints.pickle', 'rb') as f:\n",
    "    team_constraints = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed19ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e9b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/skateboard2/team_base_constraints.pickle', 'rb') as f:\n",
    "    team_base_constraints = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61993c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_base_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59446209",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/skateboard2/base_constraints.pickle', 'rb') as f:\n",
    "    base_constraints = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbfc493",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/skateboard2/BEC_constraints.pickle', 'rb') as f:\n",
    "    BEC_constraints = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c767b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEC_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2730324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1,  0,  2]]), array([[ 0,  0, -1]])]\n",
      "[0.9272952180016123]\n"
     ]
    }
   ],
   "source": [
    "# check minimum constraints\n",
    "\n",
    "# running_constraints = [np.array([[0,  0,  -1]]), np.array([[-1,  0,  2]])]\n",
    "# running_constraints = [np.array([[-1,  0,  2]]), np.array([[ 0, -1, -4]]), np.array([[ 1,  0, -4]]), np.array([[0, 1, 2]])]\n",
    "running_constraints = [np.array([[0,  0,  -1]]), np.array([[-1,  0,  0]]), np.array([[-1,  0,  2]])]\n",
    "\n",
    "\n",
    "min_constraints = BEC_helpers.remove_redundant_constraints(running_constraints, params.weights['val'], params.step_cost_flag)\n",
    "\n",
    "print(min_constraints)\n",
    "\n",
    "print(BEC_helpers.calc_solid_angles([min_constraints]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226e4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_uf = 0.75\n",
    "\n",
    "team_prior, particles_team_teacher = team_helpers.sample_team_pf(params.team_size, params.BEC['n_particles'], params.weights['val'], params.step_cost_flag, teacher_learning_factor=[teacher_uf,teacher_uf,teacher_uf], team_prior = params.team_prior, model_type = params.teacher_update_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02403019",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "teacher_uf = 0.75\n",
    "\n",
    "particles_demo = copy.deepcopy(particles_team_teacher['p1'])\n",
    "\n",
    "running_constraints = [np.array([[-1,  0,  0]]), np.array([[-1,  0,  2]])]\n",
    "\n",
    "particles_demo.update(running_constraints, teacher_uf, viz_flag=True, model_type = params.teacher_update_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d90a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_demo_strategies(path, files, file_prefix_list, runs_to_exclude_list = [], vars_filename_prefix = ''):\n",
    "\n",
    "    try:\n",
    "        with open(path + '/' + vars_filename_prefix + '_demo_strategies_subset_N15_checked2.pickle', 'rb') as f:\n",
    "            run_data_df_subset = pickle.load(f)\n",
    "        print('Opening file: ', vars_filename_prefix + '_demo_strategies_subset_N15_checked.pickle')\n",
    "\n",
    "    except:\n",
    "        run_data_list = []\n",
    "        run_data_learning_incomplete_list = []\n",
    "        analysis_run_id = 1\n",
    "\n",
    "        with open(path + '/sim_study_no_dup_N6_sample_cluster_weights_trial_data_subset_N15_checked.pickle', 'rb') as f:\n",
    "            trial_data = pickle.load(f)\n",
    "\n",
    "        unique_runs = trial_data['analysis_run_id'].unique()\n",
    "\n",
    "\n",
    "        for file in files:\n",
    "            # check if file is a valid pickle file\n",
    "            \n",
    "            for file_prefix in file_prefix_list:\n",
    "                if file_prefix in file and '.pickle' in file and '.png' not in file and 'counterfactual' not in file:\n",
    "                    run_file_flag = True\n",
    "                    break\n",
    "                else:\n",
    "                    run_file_flag = False\n",
    "\n",
    "\n",
    "            # check if file needs to be excluded\n",
    "            for runs_to_exclude in runs_to_exclude_list:\n",
    "                if runs_to_exclude in file:\n",
    "                    run_file_flag = False\n",
    "                    break\n",
    "            \n",
    "\n",
    "            if run_file_flag:\n",
    "                with open(path + '/sim_data/' + file, 'rb') as f:\n",
    "                    sim_vars = pickle.load(f)\n",
    "                print('Opening file: ', file)\n",
    "        \n",
    "                study_id = sim_vars['study_id'][0]\n",
    "                sim_run_id = sim_vars['run_no'][0]\n",
    "\n",
    "                # check if learning was completed based on sim status flag\n",
    "                if sim_vars['sim_status'].iloc[-1] == 'Teaching complete':\n",
    "                    learning_complete = True\n",
    "                else:\n",
    "                    learning_complete = False\n",
    "\n",
    "                # read counterfactuals sampled models data\n",
    "                folder = file.strip('.pickle')\n",
    "                demo_file_name = path + '/ind_sim_trials/' + folder + '/demo_gen_log.txt'\n",
    "\n",
    "                try:\n",
    "                    with open(path + '/ind_sim_trials/' + folder + '_counterfactuals.pickle', 'rb') as f:\n",
    "                        counterfactuals = pickle.load(f)\n",
    "                    print('Already compiled counterfactuals data. Reading from file. ')\n",
    "                except:\n",
    "                    if 'missing' in file:\n",
    "                        counterfactuals = read_demo_gen_log(demo_file_name, new_flag=True)\n",
    "                    else:\n",
    "                        counterfactuals = read_demo_gen_log(demo_file_name, new_flag=True)\n",
    "\n",
    "                ############## compile data\n",
    "                study_id = sim_vars['study_id'].iloc[0]\n",
    "                run_id = sim_vars['run_no'].iloc[0]\n",
    "                team_composition = str(sim_vars['team_composition'].iloc[0])\n",
    "                demo_strategy = str(sim_vars['demo_strategy'].iloc[0])\n",
    "                sim_status = str(sim_vars['sim_status'].iloc[-1])\n",
    "                min_BEC_constraints = sim_vars['min_BEC_constraints'].iloc[-1]\n",
    "                running_BEC_constraints = copy.deepcopy(params.prior)\n",
    "                print('Prior constraints: ', params.prior)\n",
    "                # filter extra sims in same file\n",
    "                kc_id_list = sim_vars['knowledge_comp_id']\n",
    "                run_start_id = []\n",
    "                for id in range(len(kc_id_list)-1):\n",
    "                    if kc_id_list[id+1] - kc_id_list[id] < 0:\n",
    "                        print(colored('Duplicate trials found for file: ' + file + '. Run id: ' + str(sim_run_id), 'red' ))\n",
    "                        run_start_id = [id+1]\n",
    "                        break\n",
    "                \n",
    "                if len(run_start_id) != 0:\n",
    "                    sim_vars = sim_vars.iloc[run_start_id[0]:].reset_index(drop=True)\n",
    "                ##################\n",
    "                    \n",
    "                for i, int_data in sim_vars.iterrows():\n",
    "                    print('interaction: ', i+1)\n",
    "                    run_data_dict = {'study_id': study_id, 'run_id': run_id, 'analysis_run_id': analysis_run_id, 'team_composition': team_composition, 'demo_strategy': demo_strategy, 'sim_status': sim_status, \\\n",
    "                                        'int_id': int_data['loop_count']}\n",
    "                    \n",
    "                    run_data_dict['kc_id'] = int_data['knowledge_comp_id']\n",
    "                    run_data_dict['min_KC_constraints'] = int_data['min_KC_constraints']\n",
    "                    print('Previous running demo constraints: ', running_BEC_constraints)\n",
    "                    running_BEC_constraints.extend(int_data['min_KC_constraints'])\n",
    "                    print('min KC constraints: ', int_data['min_KC_constraints'])\n",
    "\n",
    "                    run_data_dict['KC_constraints_area'] = BEC_helpers.calc_solid_angles([int_data['min_KC_constraints']])[0]\n",
    "                    run_data_dict['BEC_constraints_area'] = BEC_helpers.calc_solid_angles([int_data['min_BEC_constraints']])[0]\n",
    "                    print('Demo constraints: ', running_BEC_constraints)\n",
    "                    running_BEC_constraints = BEC_helpers.remove_redundant_constraints(running_BEC_constraints, params.weights['val'], params.step_cost_flag)\n",
    "                    print('Min demo constraints: ', running_BEC_constraints)\n",
    "                    run_data_dict['running_BEC_constraints'] = copy.deepcopy(running_BEC_constraints)\n",
    "                    run_data_dict['running_BEC_constraints_area'] = BEC_helpers.calc_solid_angles([running_BEC_constraints])[0]\n",
    "        \n",
    "                    if i > 0:\n",
    "                        run_data_dict['delta_BEC_constraints_area'] = running_BEC_constraints_area_prev - run_data_dict['running_BEC_constraints_area']\n",
    "                        run_data_dict['scale_reduce_BEC_constraints_area'] = running_BEC_constraints_area_prev/run_data_dict['running_BEC_constraints_area'] # area reduction (information gain) # prior BEC area (step cost is negative)\n",
    "                    else:\n",
    "                        run_data_dict['delta_BEC_constraints_area'] = 2*np.pi - run_data_dict['running_BEC_constraints_area'] # area reduction (information gain) # prior BEC area (step cost is negative)\n",
    "                        run_data_dict['scale_reduce_BEC_constraints_area'] = 2*np.pi/run_data_dict['running_BEC_constraints_area'] # area reduction (information gain) # prior BEC area (step cost is negative)\n",
    "                    \n",
    "                    \n",
    "                    BEC_knowledge_team = 0\n",
    "                    for member in int_data['BEC_knowledge_level']:\n",
    "                        run_data_dict['BEC_knowledge_level_' + member] = int_data['BEC_knowledge_level'][member]\n",
    "                        if 'p' in member:\n",
    "                            # print(int_data['BEC_knowledge_level'][member])\n",
    "                            mem_know = [float(x) for x in int_data['BEC_knowledge_level'][member]]\n",
    "                            # print(mem_know)\n",
    "                            BEC_knowledge_team += np.sum(mem_know)\n",
    "\n",
    "                    run_data_dict['BEC_knowledge_team'] = BEC_knowledge_team/3\n",
    "\n",
    "                    # find sampled counterfactuals\n",
    "                    summary_count = int_data['summary_count']\n",
    "                    counterfactual_models_list = counterfactuals[counterfactuals['length_of_summary']==summary_count]['sample_human_models'].reset_index(drop=True)\n",
    "                    counterfactual_models = counterfactual_models_list.iloc[0]\n",
    "                    counterfactual_models = [np.array(x).astype(float) for x in counterfactual_models]\n",
    "\n",
    "                    # calculate how close the sampled points are\n",
    "                    counterfactual_models_latllong = cg.cart2latlong(counterfactual_models)\n",
    "                    pairwise = metrics.pairwise.haversine_distances(counterfactual_models_latllong)\n",
    "                    # print(pairwise)\n",
    "                    run_data_dict['pairwise_dist_mean'] = np.mean(pairwise)\n",
    "                    run_data_dict['pairwise_dist_std'] = np.std(pairwise)\n",
    "                    run_data_dict['sampled_models_cluster_idxs'] = counterfactuals[counterfactuals['length_of_summary']==summary_count]['sampled_models_cluster_idxs'].iloc[0]\n",
    "                    run_data_dict['cluster_weights'] = counterfactuals[counterfactuals['length_of_summary']==summary_count]['cluster_weights'].iloc[0]\n",
    "\n",
    "                    # update end of loop vars\n",
    "                    running_BEC_constraints_area_prev = run_data_dict['running_BEC_constraints_area']\n",
    "\n",
    "                    # print(run_data_dict)\n",
    "                    if learning_complete:\n",
    "                        run_data_list.append(run_data_dict)\n",
    "                    else:\n",
    "                        run_data_learning_incomplete_list.append(run_data_dict)\n",
    "                    \n",
    "\n",
    "                    with open(path + '/ind_sim_trials/' + folder + '_counterfactuals.pickle', 'wb') as f:\n",
    "                        pickle.dump(counterfactuals, f)\n",
    "                    counterfactuals.to_csv(path + '/ind_sim_trials/' + folder + '_counterfactuals.csv')\n",
    "\n",
    "                ####################\n",
    "                if learning_complete:\n",
    "                    analysis_run_id += 1\n",
    "        \n",
    "        ################ end of reading files ##########\n",
    "        \n",
    "        run_data_df = pd.DataFrame(run_data_list)\n",
    "        run_data_incomplete_df = pd.DataFrame(run_data_learning_incomplete_list)\n",
    "\n",
    "        with open(path + '/' + vars_filename_prefix + '_demo_strategies_N15_checked.pickle', 'wb') as f:\n",
    "            pickle.dump(run_data_df, f)\n",
    "\n",
    "        run_data_df.to_csv(path + '/' + vars_filename_prefix + '_demo_strategies_N15_checked.csv')\n",
    "\n",
    "\n",
    "        with open(path + '/' + vars_filename_prefix + '_demo_strategies_learning_incomplete.pickle', 'wb') as f:\n",
    "            pickle.dump(run_data_incomplete_df, f)\n",
    "        \n",
    "        run_data_incomplete_df.to_csv(path + '/' + vars_filename_prefix + '_demo_strategies_learning_incomplete.csv')\n",
    "\n",
    "        index_to_choose = run_data_df['analysis_run_id'].isin(unique_runs).index\n",
    "        run_data_df_subset = run_data_df.iloc[index_to_choose]\n",
    "\n",
    "        with open(path + '/' + vars_filename_prefix + '_demo_strategies_subset_N15_checked.pickle', 'wb') as f:\n",
    "            pickle.dump(run_data_df_subset, f)\n",
    "\n",
    "        run_data_df_subset.to_csv(path + '/' + vars_filename_prefix + '_demo_strategies_subset_N15_checked.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf83aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Prior: ', params.prior)\n",
    "# path = 'models/augmented_taxi2'\n",
    "# files = ['03_08_sim_study_no_dup_N6_sample_cluster_weights_study_1_run_7.pickle']\n",
    "# file_prefix_list = ['03_08_sim_study_no_dup_N6_sample_cluster_weights']\n",
    "# vars_filename = 'test'\n",
    "# analyze_demo_strategies(path, files, file_prefix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ca47b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening  models/skateboard2/counterfactual_data_precomputed/model2403/cf_data_env00040.pickle\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "model = 2403\n",
    "env = 40\n",
    "filename = 'models/skateboard2/counterfactual_data_precomputed/model' + str(model) + '/cf_data_env' + str(env).zfill(5) + '.pickle'\n",
    "\n",
    "print('Opening ', filename)\n",
    "with open(filename, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82811d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_points:  500\n",
      "Point idx:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-926a1a422584>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#                 print(colored('\\033[91mInvalid point '+ str(point) + 'for env ' + str(env_idx), 'red'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraj_record\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m########  end of env loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for valid reward weights for each domain\n",
    "N = 500\n",
    "\n",
    "mdp_class_list = ['augmented_taxi2', 'skateboard2']\n",
    "\n",
    "env_list = np.arange(64)\n",
    "\n",
    "valid_env_points = {}\n",
    "\n",
    "for mdp_class in mdp_class_list:\n",
    "    \n",
    "    valid_env_points[mdp_class] = []\n",
    "\n",
    "    params.mdp_class = mdp_class\n",
    "    \n",
    "    with open('models/' + params.data_loc['BEC'] + '/team_base_constraints.pickle', 'rb') as f:\n",
    "        policy_constraints, min_subset_constraints_record, env_record, traj_record, traj_features_record, reward_record, mdp_features_record, consistent_state_count = pickle.load(f)\n",
    "\n",
    "\n",
    "    N_points = BEC_helpers.sample_human_models_uniform([], N)\n",
    "    \n",
    "    point_idx = 0\n",
    "    for point in N_points:\n",
    "        print('Point idx: ', point_idx)\n",
    "        \n",
    "        for env_idx in env_list:\n",
    "            \n",
    "#             filename = 'models/' + params.mdp_class + '/gt_policies/wt_vi_traj_params_env' + str(env_idx).zfill(5) + '.pickle'\n",
    "#             with open(filename, 'rb') as f:\n",
    "#                 wt_vi_traj_env = pickle.load(f)\n",
    "                        \n",
    "            mdp = wt_vi_traj_env[0][1].mdp\n",
    "#             mdp.set_init_state(opt_traj[0][0])\n",
    "            mdp.weights = point\n",
    "            vi_human = ValueIteration(mdp, sample_rate=1, max_iterations=100)\n",
    "            vi_human.run_vi()\n",
    "            \n",
    "            \n",
    "            if vi_human.stabilized:\n",
    "#                 print(colored('\\033[92mValid point '+ str(point) + 'for env ' + str(env_idx), 'green'))\n",
    "                valid_env_points[mdp_class].append((env_idx, point_idx, point))\n",
    "#             else:\n",
    "#                 print(colored('\\033[91mInvalid point '+ str(point) + 'for env ' + str(env_idx), 'red'))\n",
    "                            \n",
    "        ########  end of env loop\n",
    "        \n",
    "        point_idx += 1\n",
    "\n",
    "        \n",
    "        \n",
    "with open('models/' + params.mdp_class + '/valid_env_points.pickle', 'wb') as f:\n",
    "    pickle.dump(valid_env_points, f)\n",
    "        \n",
    "print(len(valid_env_points))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474b6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
